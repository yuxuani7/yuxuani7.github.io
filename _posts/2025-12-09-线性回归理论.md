---
layout: post
title: 线性回归理论
date: 2025-12-09
description: 线性回归理论推导
tags: 
    - 线性回归
    - 深度学习
    - 线性代数
---
# 线性回归

## 0. 线性模型
线性模型的基本形式为

$$
y = \mathbf{w}^\top \mathbf{x} + b
    = w_1 x_1 + w_2 x_2 + \cdots + w_d x_d + b
$$

其中：

- $d$ 表示输入特征的维度  
- $\mathbf{x} \in \mathbb{R}^d$ 为输入向量（特征向量）  
- $\mathbf{w} \in \mathbb{R}^d$ 为参数向量（权重向量）  
- $b \in \mathbb{R}$ 为偏置项  

现实中我们可以获得一组数据，假设样本数为 $n$。  
对于每个样本 $i \in \{1,\dots,n\}$，有：

$$
y_i = \mathbf{w}^\top \mathbf{x}_i + b
$$

将全部样本的输出写成向量形式：

$$
\mathbf{y}
= \begin{bmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{bmatrix}
= \begin{bmatrix}
\mathbf{w}^\top \mathbf{x}_1 + b \\
\mathbf{w}^\top \mathbf{x}_2 + b \\
\vdots \\
\mathbf{w}^\top \mathbf{x}_n + b
\end{bmatrix}
$$

记数据矩阵为：

$$
\mathbf{X}
= \begin{bmatrix}
\mathbf{x}_1^\top \\
\mathbf{x}_2^\top \\
\vdots \\
\mathbf{x}_n^\top
\end{bmatrix}
\in \mathbb{R}^{n \times d},
$$

则线性模型可以写为矩阵形式：

$$
\mathbf{y} = \mathbf{X}\mathbf{w} + b\,\mathbf{1},
$$

其中 $\mathbf{1}$ 为长度为 $n$ 的全 1 列向量。

我们需要通过给定的$\mathbf{y}$和$\mathbf{X}$，求得$\mathbf{w}$和$b$。

## 1. 损失函数

我们需要定义一个损失函数，用于刻画真实值与预测值之间的误差。

对第 $i$ 个样本，记真实值为 $y^{(i)}$，预测值为 $\hat{y}^{(i)}$，定义单样本损失为

$$
l^{(i)}(\mathbf{w}, b)
= \frac{1}{2}\bigl(\hat{y}^{(i)} - y^{(i)}\bigr)^2.
$$

由于

$$
\hat{y}^{(i)} = \mathbf{w}^\top \mathbf{x}^{(i)} + b,
$$
代入可以写成

$$
l^{(i)}(\mathbf{w}, b)
= \frac{1}{2}\bigl(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\bigr)^2.
$$

对全部 $n$ 个样本，平均损失函数（均方误差）为

$$
L(\mathbf{w}, b)
= \frac{1}{n}\sum_{i=1}^{n} l^{(i)}(\mathbf{w}, b)
= \frac{1}{2n}\sum_{i=1}^{n}
\bigl(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\bigr)^2.
$$

我们的目标是寻找一组参数 $(\mathbf{w}^\ast, b^\ast)$，使得损失函数最小化：

$$
(\mathbf{w}^\ast, b^\ast)
= \arg\min_{\mathbf{w},\,b} L(\mathbf{w}, b).
$$

记

$$
\hat{\mathbf{y}} = \mathbf{X}\mathbf{w} + b\mathbf{1}, \quad
\mathbf{y} = \begin{bmatrix} y^{(1)} \\ \vdots \\ y^{(n)} \end{bmatrix},
$$
则损失函数可以写成向量形式：

$$
L(\mathbf{w}, b)
= \frac{1}{2n}\bigl\|\hat{\mathbf{y}} - \mathbf{y}\bigr\|_2^2
= \frac{1}{2n}\bigl\|\mathbf{X}\mathbf{w} + b\mathbf{1} - \mathbf{y}\bigr\|_2^2.
$$

## 2. 解析解
线性回归的参数存在解析解。
为了求出使损失函数最小的参数 $(\mathbf{w}, b)$，我们对损失函数分别对 $\mathbf{w}$ 和 $b$ 求偏导，并令其为零。

### 2.1 对 $\mathbf{w}$ 求导

记误差为

$$
e^{(i)} = \mathbf{w}^\top\mathbf{x}^{(i)} + b - y^{(i)}.
$$

损失关于 $\mathbf{w}$ 的梯度为

$$
\nabla_{\mathbf{w}} L
= \frac{1}{n}\sum_{i=1}^{n} e^{(i)}\,\mathbf{x}^{(i)}.
$$

令梯度为零，得到：

$$
\sum_{i=1}^{n}
\bigl(\mathbf{w}^\top\mathbf{x}^{(i)} + b - y^{(i)}\bigr)\mathbf{x}^{(i)} = 0.
$$

这给出了关于 $\mathbf{w}$ 的最优条件。

### 2.2 对 $b$ 求导

关于偏置项 $b$ 的导数为

$$
\frac{\partial L}{\partial b}
= \frac{1}{n}\sum_{i=1}^{n} e^{(i)}.
$$

令其为零：

$$
\sum_{i=1}^{n}
\bigl(\mathbf{w}^\top\mathbf{x}^{(i)} + b - y^{(i)}\bigr) = 0.
$$

### 2.3 联立求解

上面两条方程分别来自  
- “误差加权的特征向量之和为零”  
- “误差之和为零”

联立这两个条件即可解得线性回归的最优参数 $(\mathbf{w}^{\*}, b^{\*})$。

最终解可以写成矩阵形式：

$$
\begin{cases}
\sum_{i=1}^{n} e^{(i)}\mathbf{x}^{(i)} = 0, \\
\sum_{i=1}^{n} e^{(i)} = 0,
\end{cases}
\quad\Longrightarrow\quad
\tilde{\mathbf{w}}^* = (\tilde{\mathbf{X}}^\top \tilde{\mathbf{X}})^{-1} \tilde{\mathbf{X}}^\top \mathbf{y}.
$$

也就是说，线性回归的最优解由“梯度为零”条件唯一确定。

## 3. 随机梯度下降（Stochastic Gradient Descent）

虽然线性回归存在解析解，但现实中大多数模型（尤其是深度神经网络）没有可直接求得的解析解。这时就需要使用梯度下降来最小化损失函数。

梯度下降的思想非常简单：在当前参数处计算损失函数的梯度，并沿着梯度的反方向更新参数，使损失不断减小。

给定学习率（步长）$\eta > 0$，参数更新为：

$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \, \nabla_{\mathbf{w}} L,
\qquad
b \leftarrow b - \eta \, \frac{\partial L}{\partial b}.
$$

对于线性回归，梯度为：

$$
\nabla_{\mathbf{w}} L = \frac{1}{n}\sum_{i=1}^{n} l^{(i)} \mathbf{x}^{(i)},
\qquad
\frac{\partial L}{\partial b} = \frac{1}{n}\sum_{i=1}^{n} l^{(i)}.
$$

### 3.1 批量梯度下降（Batch Gradient Descent）
如果每次更新都使用全部样本计算梯度，即上式中的求和包含所有 $n$ 个样本，那么称为**批量梯度下降**。  
优点是方向准确，缺点是每次更新成本高，尤其当 $n$ 很大时计算非常慢。

### 3.2 随机梯度下降（Stochastic Gradient Descent, SGD）
为了加快训练速度，可以在每次更新时只使用**一个随机样本** $(\mathbf{x}^{(i)}, y^{(i)})$ 来近似损失的梯度：

$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \, l^{(i)} \mathbf{x}^{(i)},
\qquad
b \leftarrow b - \eta \, l^{(i)}.
$$

虽然用单个样本估计的方向比较“噪声大”，但更新速度快，整体上仍然能逼近最优解。

### 3.3 小批量梯度下降（Mini-batch SGD）
实际应用中最常用的是同时使用一小部分样本（如 32、64、128）来计算梯度，即：

$$
\mathbf{w} \leftarrow \mathbf{w} - \eta \, 
\frac{1}{|\mathcal{B}|}\sum_{i \in \mathcal{B}} l^{(i)} \mathbf{x}^{(i)},
$$

其中 $\mathcal{B}$ 是一个小批量。

这种方法：
- 比批量梯度下降更快
- 比纯 SGD 更稳定
- 能很好地利用向量化计算（特别是 GPU）




